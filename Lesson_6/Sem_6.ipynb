{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHgFr0GKjjqh"
      },
      "outputs": [],
      "source": [
        "!pip install datasets -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxMb-yrojhgJ",
        "outputId": "4428bbbf-106f-44aa-8494-16f6749b4d57"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "датасеты можете брать отсюда: https://huggingface.co/datasets \\\n",
        "выбираете датасет и вставляете в load_dataset() его название \\\n",
        "есть параметр name (название датасета, если их несколько) \\\n",
        "и параметр split (train, test и validation) - если вам нужен только train, test или validation"
      ],
      "metadata": {
        "id": "1repiquM8VM_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "f-qfR0MPjiyR",
        "outputId": "70c3beac-b1fc-4092-b1ef-87ff3df34023"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset('cardiffnlp/tweet_eval', name='irony')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbSY8RFiji1F"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "модельки брать можете здесь: https://huggingface.co/models \\\n",
        "выбираем подходящую и копируем название"
      ],
      "metadata": {
        "id": "nJSCi4zr797c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_awxRMhji3l"
      },
      "outputs": [],
      "source": [
        "# загружаем модель (bert-base-uncased -> uncased - значит ей неважно, какой регистр (просто .lower() делается): эмбеддинги 'привет' и 'Привет' в одинаковых контекстах будут одинаковы)\n",
        "# и токенизатор (у берта токенизатор - WordPiece, можно почитать про него тут: https://huggingface.co/learn/llm-course/ru/chapter6/6)\n",
        "model_name = 'google-bert/bert-base-uncased'\n",
        "\n",
        "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dskh5DsFq85i"
      },
      "outputs": [],
      "source": [
        "# проверим максимальную длину в токенах для всех датасетов (чтобы понять, до какой длины делать padding и truncation)\n",
        "\n",
        "max_length_train = 0\n",
        "for example in dataset['train']:\n",
        "    max_length_train = max(max_length_train, (len(tokenizer(example['text'])['input_ids'])))\n",
        "\n",
        "max_length_test = 0\n",
        "for example in dataset['test']:\n",
        "    max_length_test = max(max_length_test, (len(tokenizer(example['text'])['input_ids'])))\n",
        "\n",
        "max_length_validation = 0\n",
        "for example in dataset['validation']:\n",
        "    max_length_validation = max(max_length_validation, (len(tokenizer(example['text'])['input_ids'])))\n",
        "\n",
        "max_length_train, max_length_test, max_length_validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qos0QsbQoAOy"
      },
      "outputs": [],
      "source": [
        "# Проверим количество параметров у нашей модели (bert-base-uncased)\n",
        "print(sum([p.numel() for p in model.parameters()])) # -> # всех параметров\n",
        "sum([p.numel() for p in model.parameters() if p.requires_grad]) # -> # параметров, которые будут обучаться"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cP4G2kI_ji6M"
      },
      "outputs": [],
      "source": [
        "# функция препроцессинга текстов\n",
        "# max_length=328 просто потому что максимальная длина немного меньше этого значения (328 взял из головы с небольшим запасом)\n",
        "def preproc(examples):\n",
        "    input_ids = tokenizer(examples['text'], max_length=328, padding='max_length', truncation=True)\n",
        "    labels = examples['label']\n",
        "    input_ids['label'] = labels\n",
        "    return input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0tAMfTcji8w"
      },
      "outputs": [],
      "source": [
        "# применяем функцию препроцессинга (токенизируем текст)\n",
        "dataset = dataset.map(preproc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmlXJIDaji_i"
      },
      "outputs": [],
      "source": [
        "# чтобы все столбцы, которые могут, стали тензорами (мы будем работать именно с тензорами)\n",
        "dataset.set_format('torch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oFbfuYyjjB4"
      },
      "outputs": [],
      "source": [
        "# создаем даталоадеры (они формируют батчи, чтобы мы потом по ним итерировались и подавали в модель сразу батч данных)\n",
        "train_loader = torch.utils.data.DataLoader(dataset['train'], batch_size=8, shuffle=True) # для трейна перемешиваем данные [shuffle=True] (чтобы избежать структурированных данных в одном батче (для разнообразия и стохастичности))\n",
        "test_loader = torch.utils.data.DataLoader(dataset['test'], batch_size=8, shuffle=False) # нет смысла шафлить данные для теста и валидации\n",
        "val_loader = torch.utils.data.DataLoader(dataset['validation'], batch_size=8, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk7azkBLjjEV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a042ee8-5cc5-404c-d674-ea3c293f15b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 328])\n",
            "torch.Size([8, 328])\n",
            "torch.Size([8])\n"
          ]
        }
      ],
      "source": [
        "# посмотрим размерности\n",
        "for batch in train_loader:\n",
        "    print(batch['input_ids'].shape) # .shape -> [batch_size, seq_len]\n",
        "    print(batch['attention_mask'].shape) # .shape -> [batch_size, seq_len]\n",
        "    print(batch['label'].shape) # .shape -> [batch_size]\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVTtYGEMqdvw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41d1bc13-9b52-4ac8-f23e-a3ea79f580f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 328, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# посмотрим размерность выхода модели (на вход подаем 'input_ids')\n",
        "model.eval()\n",
        "model(batch['input_ids']).last_hidden_state.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42M6ULIFrP1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36453032-30c2-4152-f083-695303cdb947"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# посмотрим размерность эмбеддингов в конфиге нашей модели (для того, чтобы узнать размерность, которая будет на входе в голову классификации)\n",
        "model.config.hidden_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# для примера заморозим все слои берта\n",
        "# а будем обучать только голову классификации\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "5F8DfLCNkTeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   В берте в начало каждой последовательности токенов добавляется специальный токен - [CLS], который аккумулирует в себе информацию о всей последовательности, мы и будем использовать этот токен (просто подавать [CLS] токен (размерности -> [batch_size, emb_size]) на вход в голову классификации)\n",
        "*   Можно также просто брать выход модели (размерности [batch_size, seq_len, emb_size]) и усреднять по размерности seq_len\n",
        "\n"
      ],
      "metadata": {
        "id": "uFkQZx-iB6kT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deExkvo4p_Ph"
      },
      "outputs": [],
      "source": [
        "class CLF(nn.Module):\n",
        "    def __init__(self, base_model, num_classes=2):\n",
        "        super(CLF, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        # голова классификации:\n",
        "        self.lin1 = nn.Linear(base_model.config.hidden_size, 256)\n",
        "        self.relu = nn.LeakyReLU()\n",
        "        self.lin2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attn_mask):\n",
        "        out = self.base_model(input_ids=input_ids, attention_mask=attn_mask).last_hidden_state # out.shape -> [batch_size, seq_len, emb_size]\n",
        "        # берем CLS токен:\n",
        "        cls = out[:, 0] # cls.shape -> [batch_size, emb_size]\n",
        "        return self.lin2(self.relu(self.lin1(cls))) # .shape -> [batch_size, num_classes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LHCqQknp_SR"
      },
      "outputs": [],
      "source": [
        "model = CLF(base_model=model, num_classes=2).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# проверим сколько параметров мы будем обучать\n",
        "# в нашем случае - это просто # параметров головы классификации\n",
        "sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
        "\n",
        "# их можно посчитать руками (#параметров двух линейных слоев):\n",
        "# base_model.config.hidden_size (768) * 256 + 256 (bias)\n",
        "# 256 * num_classes (2) + 2 (bias)"
      ],
      "metadata": {
        "id": "ycz-FiJKkagn",
        "outputId": "2f3bfb39-291d-4569-ffdf-eb3f39503fa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "197378"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptgpGW7fp_VB"
      },
      "outputs": [],
      "source": [
        "# так как мы обучаем только голову классификацию (параметры самого берта заморожены), то lr=5e-4\n",
        "# если хотите обучать еще и берта (разморозить все параметры), то лучше поставить поменьше, нап, lr=5e-5\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyTlgLsbp_Xn"
      },
      "outputs": [],
      "source": [
        "# прописываем классический трейн пайплайн\n",
        "\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_running_loss = 0.0\n",
        "    for batch in tqdm(train_loader, desc=f'Training, epoch: {epoch}/{epochs}'):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attn_mask = batch['attention_mask'].to(device)\n",
        "        label = batch['label'].to(device)\n",
        "\n",
        "        logits = model(input_ids, attn_mask)\n",
        "        loss = loss_fn(logits, label)\n",
        "\n",
        "        train_running_loss += loss.item()\n",
        "\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "    train_loss = train_running_loss / len(train_loader)\n",
        "    print(f'Train, Epoch: {epoch}, train_loss: {train_loss:.4f}')\n",
        "\n",
        "    model.eval()\n",
        "    eval_running_loss = 0.0\n",
        "    for batch in tqdm(val_loader, desc=f'Validation, epoch: {epoch}/{epochs}'):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attn_mask = batch['attention_mask'].to(device)\n",
        "        label = batch['label'].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_ids, attn_mask)\n",
        "            loss = loss_fn(logits, label)\n",
        "            eval_running_loss += loss.item()\n",
        "\n",
        "    eval_loss = eval_running_loss / len(val_loader)\n",
        "    print(f'Validation, Epoch: {epoch}, val_loss: {eval_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzxGXkfXjjG7",
        "outputId": "c0ae3abd-4300-43a3-e93e-0020181ae1e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 98/98 [00:16<00:00,  5.88it/s]\n"
          ]
        }
      ],
      "source": [
        "# теперь посмотрим метрики (через classification_report) на test_loader'е\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "for batch in tqdm(test_loader, desc=f'Testing'):\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attn_mask = batch['attention_mask'].to(device)\n",
        "    label = batch['label'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids, attn_mask)\n",
        "        preds = torch.argmax(logits, dim=-1) # берем класс с большим числом (логитом)\n",
        "\n",
        "    all_preds.extend(preds.cpu().numpy())\n",
        "    all_labels.extend(label.cpu().numpy())\n",
        "\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "report = classification_report(all_labels, all_preds, target_names=['class 0', 'class 1'], digits=4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(report)"
      ],
      "metadata": {
        "id": "VnLf_y-XmrOc",
        "outputId": "ad483a08-f117-4a20-f871-80b93dbfbe74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     class 0     0.8444    0.4820    0.6137       473\n",
            "     class 1     0.5233    0.8650    0.6521       311\n",
            "\n",
            "    accuracy                         0.6339       784\n",
            "   macro avg     0.6839    0.6735    0.6329       784\n",
            "weighted avg     0.7171    0.6339    0.6290       784\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ДЗ:  \n",
        "\n",
        "1.   Попробовать другие датасеты помимо этого (например, просто поменять name), но не забывайте про дисбаланс классов (чтобы модель не выучила только один класс)\n",
        "2.   Переписать трейн пайплайн на transformers.Trainer (вот документация: https://huggingface.co/docs/transformers/main_classes/trainer)\n",
        "3.   Выбить адекватное качество (поперебирать параметры, попробовать другие модели, например: https://huggingface.co/Tochka-AI/ruRoPEBert-e5-base-2k, поработать с данными (в некоторых датасетах нужна доп обработка руками))"
      ],
      "metadata": {
        "id": "7uotgcIkFL9_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zdd8m5lCjjMZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}